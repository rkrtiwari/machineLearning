---
title: "Machine Learning"
author: "Ravi Kumar Tiwari"
date: "14 June 2016"
output: pdf_document
---
## Introduction
1.  Definition: It is a method of teaching computers to make predictions based on data 

2. Types of machine learning:
    + Supervised learning
    + Unsupervised learning

3. Machine Learning in Everday life:
    + Forecasting
    + Spam filtering
    + Product recommendation
    + Fraud detection
 


\newpage

```{r, echo = FALSE, results='hide', message=FALSE, warning=FALSE}
library(caret)
library(rattle)
library(calibrate)

library(knitr)
library(party)
library(ada)
library(gbm)
```

## Linear Regression

### Working principle
Find a straight line that best describes the relationship between the dependent and the independent variables. In order to obtain the predicted value of the dependent variable, plug in the the values of the independent variable in the equation of the line. 

### Example
Build a linear model to describe the relationship between mpg (miles per gallon) and wt (weight of the car) in the mtcars dataset

```{r, echo=FALSE}
lmModel <- lm(mpg ~ wt, data = mtcars)
plot(mtcars$wt, mtcars$mpg, xlab="wt", ylab="mpg", col = "blue", pch = 20)
abline(lmModel)
```

### Codes


```{r, results='hide'}
## Build the linear model object
lmModel <- lm(mpg ~ wt, data = mtcars)

## Obtain the model parameters
sumModel <- summary(lmModel)
sumModel$coefficients

## Prediction using the model
predValue <- predict(lmModel, data.frame(wt = c(3.5, 4.2)))
predValue <- predict(lmModel, data.frame(wt = mtcars$wt))
```


### Model Assessment 

1. Visual Inspection

```{r, echo=FALSE}
plot(mtcars$wt, mtcars$mpg, xlab="wt", ylab="mpg", col = "blue", pch = 20)
abline(lmModel)
segments(mtcars$wt, predValue, mtcars$wt, mtcars$mpg, col="red")
legend(x=4.0,y=30, legend = c("residual"), lty=1, col = "red")
```

2. R-squared value

```{r}
sumModel <- summary(lmModel)
sumModel$r.squared
```

3. F-statistics

```{r}
sumModel$fstatistic
```

### Extension of linear model 

```{r, results='hide', message=FALSE, warning=FALSE}
## More than one predictors
lmModel2 <- lm(mpg ~ wt+hp+disp, data = mtcars) # wt, hp, and disp will be used as predictor
lmModel3  <- lm(mpg~ ., data = mtcars)   # All the variable will be used

## subset selection: 1) Identify the best model that contains a given number of predictors
## 2) Identify the overall best model

library(leaps)  # subset selection library
fwdSelection <- regsubsets(mpg ~ ., data = mtcars, method = "forward")
sumFwdSel <- summary(fwdSelection)
sumFwdSel$outmat  # 1) Included predictor in the Best Model when the number of predictors is fixed 
which.max(sumFwdSel$adjr2) # 2) overall best model has the highest adjusted r-squared value
```

### Output
1. Included predictors in the best model when the number of predictors are fixed
```{r}
sumFwdSel$outmat
```

2. Overall best model
```{r}
n <- which.max(sumFwdSel$adjr2)
coef(fwdSelection, n)
```


### Challenge
Use backward selection model to find the best model for mpg

\newpage

## Logistic Regression

### Working Principle
Fit the predictor values to a function whose value lies between 0 and 1. Choose a cut-off value to separates the function output values in two regions corresponding to two classes. A new observation class is decided by the region in which the function values corresponding to this observation lies.


### Example
```{r, results='hide', echo=FALSE}
inSetosa <- iris$Species == "setosa"
myIris <- iris[!inSetosa,]
myIris$Species <- factor(myIris$Species, levels = c("versicolor", "virginica"))

glmModel <- glm(Species ~ Petal.Length, data = myIris, family = binomial(link="logit"))
predValue <- predict(glmModel, myIris, type = "response")

pch <- ifelse(myIris$Species == "versicolor", 3, 4)
col <- ifelse(myIris$Species == "versicolor", "red", "green")

plot(jitter(myIris$Petal.Length, amount = 0.05),predValue, col = col, pch = pch)
abline(h=0.5)
legend(x=3, y = 1.0, legend = c("versicolor", "virginica"), pch = c(3,4), col=c("red", "green"))
```


### Codes
```{r, results='hide'}
inSetosa <- iris$Species == "setosa"
myIris <- iris[!inSetosa,]
myIris$Species <- factor(myIris$Species, levels = c("versicolor", "virginica"))
glmModel <- glm(Species ~ Petal.Length, data = myIris, family = binomial(link="logit"))
predValue <- predict(glmModel, myIris, type = "response")
```

### Model Assessment

```{r}
prediction <- ifelse(predValue > 0.5, "virginica", "versicolor")
table(prediction, myIris$Species)
```


\newpage

## Tree based algorithm
Used both for classification and regression

### Working principle
Divide the data set into several small regions such that the response variables are (nearly) homogeneous in those regions. The predictd value of a new observation is the most dominant class of the region to which the observation belongs. 


### Example
Find the decision rule to predict the species of iris dataset based on Sepal.Length, Sepal.Width, Petal.Length, and Petal.Width


```{r}
iris[c(1,100,150),]
```

### Decision Tree visualization

```{r, echo = FALSE, results='hide', message=FALSE, warning=FALSE}
library(rpart)
library(rpart.plot)

## create the tree model
set.seed(1)
inTrain <- sample(c(TRUE, FALSE), size = nrow(iris), replace = TRUE, prob = c(0.6,0.4))
trainData <- iris[inTrain,]
testData <- iris[!inTrain,1:4]
testClass <- iris[!inTrain,5]

treeModel <- rpart(Species ~ ., data = trainData)
predClass <- predict(treeModel, newdata = testData, type = "class")

## Plot the tree
par(mfrow=c(1,2))
### Decision tree Plot
rpart.plot(treeModel, type = 0)

### Decision area plot
col <- ifelse(trainData$Species == "setosa", "red", 
                   ifelse(trainData$Species== "virginica", "green", "blue"))
pty <- ifelse(trainData$Species == "setosa", 8, 
                   ifelse(trainData$Species== "virginica", 3, 4))

plot(trainData$Petal.Length, trainData$Petal.Width, col = col, 
     pch = pty, xlab = "Petal Length", ylab = "Petal Width",
     cex=0.5)

abline(v = 2.6)
segments(2.6, 1.75, 7.2, 1.75)
text(x = 1.52, y = 1.5, labels = "sesota", cex = 0.75)
text(x = 3.6, y = 2.0, labels = "virginica", cex = 0.75)
text(x = 3.9, y = 0.8, labels = "versicolor", cex = 0.75)
legend(x=4.6,y=0.75, legend = c("setosa", "versicolor" , "virginica"), 
       col = c("red", "blue", "green"), 
      pch = c(8,4,3), cex = 0.75, bty = "n")

```

### Codes
```{r, results='hide'}
## Load the required libraries
library(rpart)
library(rpart.plot)  # For decision tree visualization

## create the data partition
set.seed(1)
inTrain <- sample(c(TRUE, FALSE), size = nrow(iris), replace = TRUE, prob = c(0.6,0.4))
trainData <- iris[inTrain,]
testData <- iris[!inTrain,1:4]
testClass <- iris[!inTrain,5]

## Create the tree model
treeModel <- rpart(Species ~ ., data = trainData)

## Use the tree model to predict the class of the test data
predTrainClass <- predict(treeModel, newdata = trainData, type = "class")
predTestClass <- predict(treeModel, newdata = testData, type = "class")

## Find out the performance of the decision tree
table(predTrainClass, trainData$Species)  # Confusion Matrix
mean(predTrainClass == trainData$Species) # Prediction Accuracy

table(predTestClass, testClass)           # Confusion Matrix
mean(predTestClass == testClass)          # Prediction Accuracy
```

\newpage

### Decision tree prediction visualization

```{r, echo=FALSE, message=FALSE, warning=FALSE}
#par(mfrow=c(2,2))
layout(matrix(c(1,2,3,4), 2, 2, byrow = TRUE), heights=c(2.5,1))
## Plot train data
col <- ifelse(trainData$Species == "setosa", "red", 
                   ifelse(trainData$Species== "versicolor", "blue", "green"))

pty <- ifelse(trainData$Species == "setosa", 8, 
                   ifelse(trainData$Species== "virginica", 3, 4))

plot(trainData$Petal.Length, trainData$Petal.Width, col = col, 
     pch = pty, xlab = "Petal Length", ylab = "Petal Width",
     main = "Training Data", cex=0.5, xlim = c(0.7,7.2), ylim = c(0,2.6))
abline(v = 2.6, lwd = 0.3)
segments(2.6, 1.75, 7.5, 1.75, lwd = 0.3)
text(x = 1.52, y = 1.5, labels = "sesota", cex = 0.75)
text(x = 3.6, y = 2.0, labels = "virginica", cex = 0.75)
text(x = 3.9, y = 0.8, labels = "versicolor", cex = 0.75)
legend(x=5.0,y=0.7, legend = c("setosa", "versicolor", "virginica"), 
       col = c("red", "blue", "green"), 
      pch = c(8,4,3), cex = 0.5, bty = "n")

# Plot test data
predClass <- predict(treeModel, newdata = testData, type = "class")
col <- ifelse(testClass == "setosa", "red", 
                   ifelse(testClass == "versicolor", "blue", "green"))
pty <- ifelse(testClass == "setosa", 8, 
                   ifelse(testClass== "virginica", 3, 4))

plot(testData$Petal.Length, testData$Petal.Width, col = col, 
     pch = pty, cex = 0.5, xlab = "Petal Length", ylab = "Petal Width",
     main = "Test Data", xlim = c(0.7,7.2), ylim = c(0,2.6))
abline(v = 2.6, lwd = 0.3)
segments(2.6, 1.75, 7.5, 1.75, lwd = 0.3)
text(x = 1.52, y = 1.5, labels = "setosa", cex = 0.75)
text(x = 3.6, y = 2.0, labels = "virginica", cex = 0.75)
text(x = 3.9, y = 0.8, labels = "versicolor", cex = 0.75)
legend(x=5.0,y=0.7, legend = c("setosa", "versicolor", "virginica"), 
       col = c("red", "blue", "green"),
       pch = c(8,4,3), cex = 0.5, bty = "n")

library(PerformanceAnalytics)

textplot(table(predTrainClass, trainData$Species))
textplot(table(predTestClass, testClass))
par(mfrow=c(1,1))
```


### Add some challenge

### Advantages of decision tree
Easy to interpret

### Problem with the decision tree
Lower prediction accuracy

### Solution
Aggregate many decision trees (bagging, random forest, boosting)

## random Forest
Need to decorrelate the trees. Making it more accurate

### ntree
```{r, echo=FALSE}
include_graphics("rf.png")
```

### mtry
Decorrelate the trees
a random sample of m predictors is chosen as split candidates from the full set of 
p predictors.

```{r, echo=FALSE, fig.width=12}
include_graphics("mtry.jpg")
```

### random forest example
```{r, message=FALSE, warning=FALSE}
library(randomForest)
rfModel <- randomForest(Species ~ ., data=trainData, mtry=3, ntree=15)
predClass <- predict(rfModel, newdata = testData)
table(predClass, testClass)
varImpPlot(rfModel)
```

### Add some challenge

## Boosting

### Illustration
```{r, echo=FALSE}
include_graphics("boosting.png")
```

### example
```{r}
#gbmModel <- gbm(Species ~ ., data = trainData, distribution = "multinomial", 
#                n.trees = 20)
#predict(gbmModel, newdata = testData, single.tree = TRUE, n.trees = 20, type = "response")


# boost.boston=gbm(medv~.,data=Boston[train,],distribution= # #"gaussian",n.trees=5000,interaction.depth=4)
```

The argument n.teees = 5000 indicates that we want 5000 trees, and the option interaction.depth = 4 limits the depth of each tree

\newpage

## knn
make sure, you scale the data and also with an example tell why  it is important to 
scale the data

### Working principle
It assumes that the members of a given class have similar characteristics. So, a given observation is assigned the class of its nearest neighbours (number of nearest neighbour to be decided by the user)

### Example

```{r, echo = FALSE, message=FALSE, warning=FALSE}
library(class)
myIris <- iris[,3:5]

set.seed(100)
inTrain <- sample(c(TRUE, FALSE), size = nrow(myIris), replace = TRUE, prob = c(0.2,0.8))
trainData <- myIris[inTrain,1:2]
trainClass <- myIris[inTrain,3]
testData <- myIris[-inTrain,1:2]
testClass <- myIris[-inTrain,3]

color <- ifelse(trainClass=="setosa", "red", ifelse(trainClass=="versicolor", "green",
                "blue"))

plot(trainData$Petal.Length, trainData$Petal.Width, pch = 19, col = color,
     xlab = "Petal.Length", ylab = "Petal.Width")
legend(x = 1.2, y = 2.5, legend = c("setosa", "versicolor", "virginica", "test data"),
       col = c("red", "green", "blue","black"), pch = c(19,19,19,8), bty = "n", cex=0.75)


testData <- data.frame(Petal.Length = 5.0, Petal.Width = 1.0)
predClass <- knn(trainData, testData, cl = trainClass, k = 3)
#table(predClass, testClass)

color <- ifelse(predClass=="setosa", "red", ifelse(predClass=="versicolor", "green",
                "blue"))

pType = ifelse(predClass == testClass, 17, 4)
points(testData$Petal.Length, testData$Petal.Width, pch = 8, col = color)

newData <- rbind(testData, trainData)
distM <- as.matrix(dist(newData))
rowNames <- names(sort(distM[,1]))[2:4]
nn <- trainData[rowNames,]

i = 1
for (i in 1:3){
  segments(testData[,1], testData[,2], nn[i,1], nn[i,2], lty = 2 )
}


testData <- data.frame(Petal.Length = 2.0, Petal.Width = 0.9)
predClass <- knn(trainData, testData, cl = trainClass, k = 3)


color <- ifelse(predClass=="setosa", "red", ifelse(predClass=="versicolor", "green",
                "blue"))

pType = ifelse(predClass == testClass, 17, 4)
points(testData$Petal.Length, testData$Petal.Width, pch = 8, col = color)

newData <- rbind(testData, trainData)
distM <- as.matrix(dist(newData))
rowNames <- names(sort(distM[,1]))[2:4]
nn <- trainData[rowNames,]

i = 1
for (i in 1:3){
  segments(testData[,1], testData[,2], nn[i,1], nn[i,2], lty = 2 )
}
```

### codes

```{r, message=FALSE, warning=FALSE}
library(class)
myIris <- iris[,3:5]

set.seed(100)
inTrain <- sample(c(TRUE, FALSE), size = nrow(myIris), replace = TRUE, prob = c(0.2,0.8))
trainData <- myIris[inTrain,1:2]
trainClass <- myIris[inTrain,3]
testData <- myIris[!inTrain,1:2]
testClass <- myIris[!inTrain,3]

predClass <- knn(trainData, testData, cl = trainClass, k = 3)
table(predClass, testClass)
```

\newpage

## clustering example
### kmeans clustering
```{r, echo=FALSE, results='hide'}
set.seed(100)
index <- sample(c(TRUE, FALSE), nrow(iris), p = c(0.2, 0.8), replace = TRUE)
myIris <- iris[index,3:4]
group <- iris$Species[index]
set.seed(100)
predGroup <- kmeans(myIris, centers = 3, nstart = 10)
predGroupC <- ifelse(predGroup$cluster==1, "setosa", ifelse(predGroup$cluster==2, 
                                                   "versicolor", "virginnica"))
predGroupC <- factor(predGroupC)
table(predGroupC, group)

par(mfrow = c(1,2))
col <- ifelse(predGroupC == "setosa", "green", 
              ifelse(predGroupC=="versicolor", "red", "blue"))
plot(myIris$Petal.Length, myIris$Petal.Width, pch = 19, col = col)
legend(x=1,y=2.5, legend = c("group1", "group2", "group3"), 
       col = c("green", "red", "blue"), pch = 19, y.intersp=0.75, cex = 0.75,
       bty = "n")

col <- ifelse(group == "setosa", "green", ifelse(group=="versicolor", "red", "blue"))
plot(myIris$Petal.Length, myIris$Petal.Width, pch = 17, col = col)
legend(x=1,y=2.5, legend = c("setosa", "versicolor", "virginica"), 
       col = c("green", "red", "blue"), pch = 17, y.intersp=0.75, cex = 0.75,
       bty = "n")
par(mfrow = c(1,1))
```

### Codes
```{r}
set.seed(100)
index <- sample(c(TRUE, FALSE), nrow(iris), p = c(0.2, 0.8), replace = TRUE)
myIris <- iris[index,3:4]
group <- iris$Species[index]
set.seed(100)
predGroup <- kmeans(myIris, centers = 3, nstart = 10)
predGroupC <- ifelse(predGroup$cluster==1, "setosa", ifelse(predGroup$cluster==2, 
                                                   "versicolor", "virginnica"))
predGroupC <- factor(predGroupC)
table(predGroupC, group)
```



### Hierarchichal Clustering

```{r, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(4)
index <- sample(c(TRUE, FALSE), nrow(iris), p = c(0.05, 0.95), replace = TRUE)
myIris <- iris[index,3:4]
disM <- dist(myIris)
irisClust <- hclust(disM)

clusters <- cutree(irisClust, k = 3)
clusters <- ifelse(clusters==1, "setosa", ifelse(clusters==2, 
                                                   "versicolor", "virginnica"))
clusters <- factor(clusters)
col <- ifelse(clusters == "setosa", "green", 
              ifelse(clusters=="versicolor", "red", "blue"))


par(mfrow = c(2,2), mar = c(0.2,2,2,2))
library(sparcl)
ColorDendrogram(irisClust, y = col, labels = names(clusters), main = "Dendogram",   branchlength = 5)

abline(h=1.5)
plot(myIris$Petal.Length, myIris$Petal.Width, pch = " ",
     xlab= "Petal.Length", ylab = "Petal.Width")
text(myIris$Petal.Length, myIris$Petal.Width, labels = which(index==TRUE), 
     cex = 0.4, col = col)



plot(myIris$Petal.Length, myIris$Petal.Width, pch = 19, col = col)
legend(x=1,y=2.5, legend = c("group1", "group2", "group3"), 
       col = c("green", "red", "blue"), pch = 19, y.intersp=0.5, 
       cex = 0.75, bty = "n")

col <- ifelse(group == "setosa", "green", ifelse(group=="versicolor", "red", "blue"))
plot(myIris$Petal.Length, myIris$Petal.Width, pch = 17, col = col)
legend(x=1,y=2.5, legend = c("setosa", "versicolor", "virginica"), 
       col = c("green", "red", "blue"), pch = 17, y.intersp=0.5, 
       cex = 0.75, bty = "n")

```

#### Codes

```{r}
set.seed(4)
index <- sample(c(TRUE, FALSE), nrow(iris), p = c(0.05, 0.95), replace = TRUE)
myIris <- iris[index,3:4]
disM <- dist(myIris)
irisClust <- hclust(disM)
clusters <- cutree(irisClust, k = 3)
```

## Cross-validation

```{r, echo = FALSE}
drawPoly <- function(x,y, col){
  polygon(c(x,x+1,x+1,x, x), c(y,y,y+0.5,y+0.5,y), col = col)
}


plot(1, type="n", axes=F, xlab="", ylab="", xlim = c(0.5,6), ylim= c(0,8),
     main = "5 fold cross validation illustration")
for (i in 1:5){
  for (j in 1:5) {
    col <- ifelse(i==j, "red", "green")
    drawPoly(i,j, col)
  }
}

drawPoly(1,6, "red")
drawPoly(1,7, "green")

for(i in 1:5){
  text(i+0.5, 0.5, label = paste("k = ", i, sep = ""))
}

for(i in 1:5){
  text(0.5, i+0.25, label = 6-i)
}

text(3, 7.3, label = "Training Data")
text(3, 6.3, label = "Test Data")
```

\newpage

## Bay
's theorem
```{r}
#head(Titanic)
```


## SVM

### Working Principle
It classifies a test observation depending on which side of a hyperplane it lies. The hyperplabe is chosen to correctly separate most of the training observations into two classes

### Example

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(e1071)
inSetosa <- iris$Species == "setosa"
myIris <- iris[!inSetosa, c("Petal.Length", "Petal.Width", "Species")]
myIris$Species <- factor(myIris$Species, levels = c("versicolor", "virginica"))
svmModel <- svm(Species ~ ., data = myIris, kernal = "linear", 
                scale = FALSE)

sym <- ifelse(myIris$Species == "versicolor", 15, 16)
col <- ifelse(myIris$Species == "versicolor", "red", "green")

x1val <- seq(0.5,8,length.out = 200)
x2val <- seq(0.5,8, length.out = 200)

xval <- data.frame(expand.grid(x1val, x2val))
names(xval) <- c("Petal.Length", "Petal.Width")
yval <- predict(svmModel, xval)
pcol <- ifelse(yval=="versicolor", "red", "green")
plot(myIris$Petal.Length, myIris$Petal.Width, pch = sym, col = col, asp = 1,
     main = "Linear Decision Boundary")
points(xval, col = pcol, cex = 0.02)

legend(x=3, y = 2.5, legend = c("versicolor", "virginica"), pch = c(15,16), col = c("red", "green"), bty = "n")
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(e1071)
inSetosa <- iris$Species == "setosa"
myIris <- iris[!inSetosa, c("Petal.Length", "Petal.Width", "Species")]
myIris$Species <- factor(myIris$Species, levels = c("versicolor", "virginica"))
svmModel <- svm(Species ~ ., data = myIris, kernal = "radial", cost = 1e5,
                scale = FALSE)

sym <- ifelse(myIris$Species == "versicolor", 15, 16)
col <- ifelse(myIris$Species == "versicolor", "red", "green")

x1val <- seq(0.5,8,length.out = 200)
x2val <- seq(0.5,8, length.out = 200)

xval <- data.frame(expand.grid(x1val, x2val))
names(xval) <- c("Petal.Length", "Petal.Width")
yval <- predict(svmModel, xval)
pcol <- ifelse(yval=="versicolor", "red", "green")
plot(myIris$Petal.Length, myIris$Petal.Width, pch = sym, col = col, asp = 1,
     main = "Non-Linear Decision Boundary")
points(xval, col = pcol, cex = 0.02)

legend(x=3, y = 2.5, legend = c("versicolor", "virginica"), pch = c(15,16), col = c("red", "green"), bty = "n")
```



### Codes 
```{r, message=FALSE, warning=FALSE, results='hide'}
library(e1071)
inSetosa <- iris$Species == "setosa"
myIris <- iris[!inSetosa, c("Petal.Length", "Petal.Width", "Species")]
myIris$Species <- factor(myIris$Species, levels = c("versicolor", "virginica"))
svmModel <- svm(Species ~ ., data = myIris, kernal = "linear", 
                scale = FALSE)
summary(svmModel)
prediction <- predict(svmModel, myIris[, 1:2])
table(prediction, myIris$Species)
```

### Assessment
```{r}
prediction <- predict(svmModel, myIris[, 1:2])
table(prediction, myIris$Species)
mean(prediction==myIris$Species)
```


