---
title: "Machine Learning"
author: "Ravi Kumar Tiwari"
date: "14 June 2016"
output: pdf_document
---


```{r, echo = TRUE, results='hide', message=FALSE, warning=FALSE}
library(caret)
library(rpart.plot)
library(rattle)
library(calibrate)
library(randomForest)
library(e1071)
library(class)
library(knitr)
library(party)
```

## Decision Tree Example
### Problem Description
Given a data set that contains some observation and corresponding class label, can a machine learning algorithm be trained to determine the class label of any data set (not necessarily the data that was used for training) from its observation

### Solution using decision tree
```{r}
head(iris)
```

Create data partition
```{r}
set.seed(100)
inTrain <- createDataPartition(iris$Species, p = 0.6, list = FALSE)
trainData <- iris[inTrain,]
testData <- iris[-inTrain,]
```

Build a decision tree model and use it for prediction on test data set
```{r}
treeModel <- train(Species ~ ., data = trainData, method = "rpart")
preClass <- predict(treeModel, newdata = testData)
cMatrix <- confusionMatrix(preClass, testData$Species)
cMatrix$table
```

Look at what are the important variables
```{r}
varImp(treeModel)
```

Visualization of the decision tree

```{r, echo=FALSE}
fancyRpartPlot(treeModel$finalModel)  
```

\newpage
Visualization of the decision tree

```{r, echo=FALSE}
par(mfrow=c(1,2))
preClassType <- ifelse(trainData$Species == "setosa", 0, 
                   ifelse(trainData$Species== "versicolor", 6, 2))
col <- ifelse(trainData$Species == "setosa", "red", 
                   ifelse(trainData$Species== "versicolor", "blue", "green"))
predClass <- predict(treeModel, newdata = trainData)
predResult <- ifelse(predClass == trainData$Species, "green", "red")


plot(trainData$Petal.Length, trainData$Petal.Width, col = predResult, 
     pch = preClassType, xlab = "Petal Length", ylab = "Petal Width",
     main = "Training Data")
abline(v = 2.4)
segments(2.4, 1.7, 7.2, 1.7 )
text(x = 1.5, y = 1.5, labels = "sesota")
text(x = 3.9, y = 2.0, labels = "virginica")
text(x = 3.9, y = 0.5, labels = "versicolor")
legend(x=5,y=0.5, legend = c("correct", "incorrect"), col = c("green", "red"), 
      lty = c(1,1), cex = 0.50)

predClass <- predict(treeModel, newdata = testData)
preClassType <- ifelse(testData$Species == "setosa", 0, 
                   ifelse(testData$Species== "versicolor", 6, 2))
predResult <- ifelse(predClass == testData$Species, "green", "red")

plot(testData$Petal.Length, testData$Petal.Width, col = predResult, 
     pch = preClassType, cex = 1.2, xlab = "Petal Length", ylab = "Petal Width",
     main = "Test Data")
abline(v = 2.4)
segments(2.4, 1.7, 7.2, 1.7 )
text(x = 1.5, y = 1.5, labels = "setosa")
text(x = 3.9, y = 2.0, labels = "virginica")
text(x = 3.9, y = 0.5, labels = "versicolor")
legend(x=4.5,y=1.0, legend = c("setosa", "versicolor", "virginica"), 
       pch = c(0,6,2), cex = 0.75)

par(mfrow=c(1,1))
```

## random Forest


### ntree
```{r, echo=FALSE}
include_graphics("rf.png")
```

### mtry
```{r, echo=FALSE, fig.width=12}
include_graphics("mtry.jpg")
```

### random forest example
```{r}
set.seed(100)
inTrain <- createDataPartition(iris[,5], p = 0.6, list=FALSE)
trainData <- iris[inTrain,]
testData <- iris[-inTrain, 1:4]
testClass <- iris[-inTrain, 5]

rfModel <- randomForest(Species ~ ., data=iris, mtry=3, ntree=10)
predClass <- predict(rfModel, newdata = testData)
table(predClass, testClass)
```


## knn2
```{r}
myIris <- iris[,3:5]
head(myIris)
nI <- nrow(myIris)
ind <- sample(1:nI, 0.8*nI)
trainData <- myIris[ind, 1:2]
trainClass <- myIris[ind, 3]
testData <- myIris[-ind, 1:2]
testClass <- myIris[-ind, 3]
preClass <- knn(trainData, testData, cl = trainClass, k = 2)
table(preClass, testClass)

color <- ifelse(trainClass=="setosa", "red", ifelse(trainClass=="versicolor", "green",
                "blue"))

plot(trainData$Petal.Length, trainData$Petal.Width, pch = 19, col = color,
     xlab = "Petal.Length", ylab = "Petal.Width")
legend(x = 1, y = 2.5, legend = c("Setosa", "versicolor", "virginica"),
       col = c("red", "green", "blue"), pch = 19)

color <- ifelse(preClass=="setosa", "red", ifelse(preClass=="versicolor", "green",
                "blue"))

pType = ifelse(preClass == testClass, 17, 4)
points(testData$Petal.Length, testData$Petal.Width, pch = pType, col = color)
```


\newpage

## clustering example
### kmeans clustering
```{r}
myIris <- iris[3:4]
group <- iris$Species
predGroup <- kmeans(myIris, centers = 3)
predGroupC <- ifelse(predGroup$cluster==2, "setosa", ifelse(predGroup$cluster==3, 
                                                   "versicolor", "virginnica"))
predGroupC <- factor(predGroupC)
table(predGroupC, group)

par(mfrow = c(1,2))
plot(myIris$Petal.Length, myIris$Petal.Width, pch = 19, col = predGroupC)
legend(x=1,y=2.5, legend = c("group1", "group2", "group3"), 
       col = c("green", "black", "red"), pch = 19, y.intersp=0.5, cex = 0.75)

plot(myIris$Petal.Length, myIris$Petal.Width, pch = 17, col = group)
legend(x=1,y=2.5, legend = c("setosa", "versicolor", "virginica"), 
       col = c("green", "black", "red"), pch = 19, y.intersp=0.5, cex = 0.75)
par(mfrow = c(1,1))
```

### hierarchichal clustering
```{r}
group <- iris$Species
disM <- dist(myIris)
irisClust <- hclust(disM)
clusters <- cutree(irisClust, k = 3)

clusters <- ifelse(clusters==1, "setosa", ifelse(clusters==2, 
                                                   "virginnica", "versicolor"))
clusters <- factor(clusters)


par(mfrow = c(1,2))
plot(myIris$Petal.Length, myIris$Petal.Width, pch = 19, col = predGroupC)
legend(x=1,y=2.5, legend = c("group1", "group2", "group3"), 
       col = c("green", "black", "red"), pch = 19, y.intersp=0.5, cex = 0.75)

plot(myIris$Petal.Length, myIris$Petal.Width, pch = 17, col = group)
legend(x=1,y=2.5, legend = c("setosa", "versicolor", "virginica"), 
       col = c("green", "black", "red"), pch = 19, y.intersp=0.5, cex = 0.75)
par(mfrow = c(1,1))
```



## Cross-validation

```{r, echo = FALSE}
drawPoly <- function(x,y, col){
  polygon(c(x,x+1,x+1,x, x), c(y,y,y+0.5,y+0.5,y), col = col)
}


plot(1, type="n", axes=F, xlab="", ylab="", xlim = c(0.5,6), ylim= c(0,8),
     main = "5 fold cross validation illustration")
for (i in 1:5){
  for (j in 1:5) {
    col <- ifelse(i==j, "red", "green")
    drawPoly(i,j, col)
  }
}

drawPoly(1,6, "red")
drawPoly(1,7, "green")

for(i in 1:5){
  text(i+0.5, 0.5, label = paste("k = ", i, sep = ""))
}

for(i in 1:5){
  text(0.5, i+0.25, label = 6-i)
}

text(3, 7.3, label = "Training Data")
text(3, 6.3, label = "Test Data")
```

\newpage




```{r}
#rfModel <- randomForest(Species ~ . , data = trainData, ntree = 3)
```

## knn
```{r}
head(trees)
set.seed(100)
index <- sample(nrow(iris), 0.6*nrow(iris))
p <- knn(iris[index, 1:4], iris[-index, 1:4], iris[index, 5], 1)
#data.frame(iris[-index, 5], p)
table(iris[-index, 5], p)
## show cross validation
## show parameter selection
## show visualization
```


## knn2
```{r}
head(trees)
set.seed(100)
index <- sample(nrow(trees), 0.6*nrow(trees))
p <- knn(trees[index, 1:2], trees[-index, 1:2], iris[index, 3], 4)
#data.frame(trees[-index, 3], p)
```


## baye's theorem
```{r}
#head(Titanic)
```


## svm
```{r}
#svmModel <- svm()
```







