---
title: "Machine Learning"
author: "Ravi Kumar Tiwari"
date: "14 June 2016"
output: pdf_document
---


```{r, echo = TRUE, results='hide', message=FALSE, warning=FALSE}
library(caret)
library(rpart.plot)
library(rattle)
library(calibrate)
library(randomForest)
library(e1071)
library(class)
library(knitr)
library(party)
library(ada)
library(gbm)
library(leaps)
```

## Regression

Used for predicting continuous variable

1. Code to Build the model object

```{r}
lmModel <- lm(mpg ~ wt, data = mtcars)
```

2. Code to obtain the model parameters

```{r}
sumModel <- summary(lmModel)
sumModel$coefficients
```

3. Prediction of response using the regression model

```{r}
predValue <- predict(lmModel, data.frame(wt = mtcars$wt))
```

4. Assessing the accuracy of the model

  + Visual Inspection

```{r, echo=FALSE}
plot(mtcars$wt, mtcars$mpg, xlab="wt", ylab="mpg", col = "blue")
abline(lmModel)
segments(mtcars$wt, predValue, mtcars$wt, mtcars$mpg, col="red")
legend(x=4.0,y=30, legend = c("residual"), lty=1, col = "red")
```

  + R-squared value

```{r}
sumModel <- summary(lmModel)
sumModel$r.squared

```

  + F-statistics

```{r}
sumModel$fstatistic
```

5. Code to build linear regression model using multiple predictors

```{r}
lmModel2 <- lm(mpg ~ wt+hp+disp, data = mtcars) # wt, hp, and disp will be used as predictor
lmModel3  <- lm(mpg~ ., data = mtcars)   # All the variable will be used
```

6. Code to do subset selection

identifies the best model that contains a given number of predictors, where best is quantified using RSS

```{r}
fwdSelection <- regsubsets(mpg ~ ., data = mtcars, method = "forward")
sumFwdSel <- summary(fwdSelection)
names(sumFwdSel)
sumFwdSel$outmat
sumFwdSel$rsq
which.max(sumFwdSel$adjr2)
coef(fwdSelection,6)
```

7. Challenge
 + Use backward selection model to find the best model for mpg

## Tree based algorithm
Used both for classification and regression

### Problem Description
Use Iris dataset to predict the Species based on Sepal.Length, Sepal.Width, Petal.Length,
Petal.Width


```{r}
head(iris)
```


```{r, echo=FALSE, results='hide'}
set.seed(100)
inTrain <- createDataPartition(iris$Species, p = 0.6, list = FALSE)
trainData <- iris[inTrain,]
testData <- iris[-inTrain,1:4]
testClass <- iris[-inTrain,5]

treeModel <- train(Species ~ ., data = trainData, method = "rpart")
predClass <- predict(treeModel, newdata = testData)
cMatrix <- confusionMatrix(predClass, testClass)
cMatrix$table

varImp(treeModel)
```

Decision tree: splitting rule

```{r, echo=FALSE}
fancyRpartPlot(treeModel$finalModel)  
```

\newpage

Decision tree: partitioned area

```{r, echo=FALSE}
par(mfrow=c(1,2))
preClassType <- ifelse(trainData$Species == "setosa", 3, 
                   ifelse(trainData$Species== "versicolor", 4, 8))
col <- ifelse(trainData$Species == "setosa", "red", 
                   ifelse(trainData$Species== "versicolor", "blue", "green"))
predClass <- predict(treeModel, newdata = trainData)
predResult <- ifelse(predClass == trainData$Species, "green", "red")


plot(trainData$Petal.Length, trainData$Petal.Width, col = predResult, 
     pch = preClassType, xlab = "Petal Length", ylab = "Petal Width",
     main = "Training Data", cex=0.5)
abline(v = 2.4)
segments(2.4, 1.7, 7.2, 1.7 )
text(x = 1.52, y = 1.5, labels = "sesota", cex = 0.75)
text(x = 3.6, y = 2.0, labels = "virginica", cex = 0.75)
text(x = 3.9, y = 0.8, labels = "versicolor", cex = 0.75)
legend(x=4.6,y=0.5, legend = c("correct", "incorrect"), col = c("green", "red"), 
      pch = 20, cex = 0.75, bty = "n")

predClass <- predict(treeModel, newdata = testData)
preClassType <- ifelse(testClass == "setosa", 3, 
                   ifelse(testClass== "versicolor", 4, 8))
predResult <- ifelse(predClass == testClass, "green", "red")

plot(testData$Petal.Length, testData$Petal.Width, col = predResult, 
     pch = preClassType, cex = 0.5, xlab = "Petal Length", ylab = "Petal Width",
     main = "Test Data")
abline(v = 2.4)
segments(2.4, 1.7, 7.2, 1.7 )
text(x = 1.52, y = 1.5, labels = "setosa", cex = 0.75)
text(x = 3.6, y = 2.0, labels = "virginica", cex = 0.75)
text(x = 3.9, y = 0.8, labels = "versicolor", cex = 0.75)
legend(x=4.5,y=0.6, legend = c("setosa", "versicolor", "virginica"), 
       pch = c(3,4,8), cex = 0.75, bty = "n")

par(mfrow=c(1,1))
```


Create data partition
```{r}
set.seed(100)
inTrain <- createDataPartition(iris$Species, p = 0.6, list = FALSE)
trainData <- iris[inTrain,]
testData <- iris[-inTrain,1:4]
testClass <- iris[-inTrain,5]
```

Build a decision tree model and use it for prediction on test data set
```{r}
treeModel <- train(Species ~ ., data = trainData, method = "rpart")
predClass <- predict(treeModel, newdata = testData)
cMatrix <- confusionMatrix(predClass, testClass)
cMatrix$table
```

Look at what are the important variables
```{r}
varImp(treeModel)
```

### Add some challenge

### Advantages of decision tree
Easy to interpret

### Problem with the decision tree
Lower prediction accuracy

### Solution
Aggregate many decision trees (bagging, random forest, boosting)

## random Forest
Need to decorrelate the trees. Making it more accurate

### ntree
```{r, echo=FALSE}
include_graphics("rf.png")
```

### mtry
Decorrelate the trees
a random sample of m predictors is chosen as split candidates from the full set of 
p predictors.

```{r, echo=FALSE, fig.width=12}
include_graphics("mtry.jpg")
```

### random forest example
```{r}
rfModel <- randomForest(Species ~ ., data=trainData, mtry=3, ntree=15)
predClass <- predict(rfModel, newdata = testData)
table(predClass, testClass)
varImpPlot(rfModel)
```

### Add some challenge

## Boosting

### Illustration
```{r, echo=FALSE}
include_graphics("boosting.png")
```

### example
```{r}
#gbmModel <- gbm(Species ~ ., data = trainData, distribution = "multinomial", 
#                n.trees = 20)
#predict(gbmModel, newdata = testData, single.tree = TRUE, n.trees = 20, type = "response")


# boost.boston=gbm(medv~.,data=Boston[train,],distribution= # #"gaussian",n.trees=5000,interaction.depth=4)
```

The argument n.teees = 5000 indicates that we want 5000 trees, and the option interaction.depth = 4 limits the depth of each tree

## knn
```{r}
myIris <- iris[,3:5]
head(myIris)

inTrain <- createDataPartition(myIris$Species, p = 0.6, list = FALSE)
trainData <- myIris[inTrain,1:2]
trainClass <- myIris[inTrain,3]
testData <- myIris[-inTrain,1:2]
testClass <- myIris[-inTrain,3]

predClass <- knn(trainData, testData, cl = trainClass, k = 3)
table(predClass, testClass)
```


```{r, echo=FALSE}
color <- ifelse(trainClass=="setosa", "red", ifelse(trainClass=="versicolor", "green",
                "blue"))

plot(trainData$Petal.Length, trainData$Petal.Width, pch = 19, col = color,
     xlab = "Petal.Length", ylab = "Petal.Width")
legend(x = 1, y = 2.5, legend = c("Setosa", "versicolor", "virginica"),
       col = c("red", "green", "blue"), pch = 19)

color <- ifelse(predClass=="setosa", "red", ifelse(predClass=="versicolor", "green",
                "blue"))

pType = ifelse(predClass == testClass, 17, 4)
points(testData$Petal.Length, testData$Petal.Width, pch = pType, col = color)
```


\newpage

## clustering example
### kmeans clustering
```{r, echo=FALSE}
set.seed(100)
index <- sample(c(TRUE, FALSE), nrow(iris), p = c(0.2, 0.6), replace = TRUE)
myIris <- iris[index,3:4]
group <- iris$Species[index]
set.seed(100)
predGroup <- kmeans(myIris, centers = 3, nstart = 10)
predGroupC <- ifelse(predGroup$cluster==1, "setosa", ifelse(predGroup$cluster==2, 
                                                   "versicolor", "virginnica"))
predGroupC <- factor(predGroupC)
table(predGroupC, group)

par(mfrow = c(1,2))
col <- ifelse(predGroupC == "setosa", "green", 
              ifelse(predGroupC=="versicolor", "red", "blue"))
plot(myIris$Petal.Length, myIris$Petal.Width, pch = 19, col = col)
legend(x=1,y=2.5, legend = c("group1", "group2", "group3"), 
       col = c("green", "red", "blue"), pch = 19, y.intersp=0.75, cex = 0.75,
       bty = "n")

col <- ifelse(group == "setosa", "green", ifelse(group=="versicolor", "red", "blue"))
plot(myIris$Petal.Length, myIris$Petal.Width, pch = 17, col = col)
legend(x=1,y=2.5, legend = c("setosa", "versicolor", "virginica"), 
       col = c("green", "red", "blue"), pch = 17, y.intersp=0.75, cex = 0.75,
       bty = "n")
par(mfrow = c(1,1))
```

### hierarchichal clustering
```{r, echo=FALSE}
disM <- dist(myIris)
irisClust <- hclust(disM)
clusters <- cutree(irisClust, k = 3)

clusters <- ifelse(clusters==1, "setosa", ifelse(clusters==2, 
                                                   "versicolor", "virginnica"))
clusters <- factor(clusters)

col <- ifelse(clusters == "setosa", "green", 
              ifelse(clusters=="versicolor", "red", "blue"))
par(mfrow = c(1,2))
plot(myIris$Petal.Length, myIris$Petal.Width, pch = 19, col = col)
legend(x=1,y=2.5, legend = c("group1", "group2", "group3"), 
       col = c("green", "red", "blue"), pch = 19, y.intersp=0.5, 
       cex = 0.75, bty = "n")

col <- ifelse(group == "setosa", "green", ifelse(group=="versicolor", "red", "blue"))
plot(myIris$Petal.Length, myIris$Petal.Width, pch = 17, col = col)
legend(x=1,y=2.5, legend = c("setosa", "versicolor", "virginica"), 
       col = c("green", "red", "blue"), pch = 17, y.intersp=0.5, 
       cex = 0.75, bty = "n")
par(mfrow = c(1,1))
plot(irisClust)
plot(myIris$Petal.Length, myIris$Petal.Width, pch = ".")
text(myIris$Petal.Length, myIris$Petal.Width, labels = which(index==TRUE))
```


## Cross-validation

```{r, echo = FALSE}
drawPoly <- function(x,y, col){
  polygon(c(x,x+1,x+1,x, x), c(y,y,y+0.5,y+0.5,y), col = col)
}


plot(1, type="n", axes=F, xlab="", ylab="", xlim = c(0.5,6), ylim= c(0,8),
     main = "5 fold cross validation illustration")
for (i in 1:5){
  for (j in 1:5) {
    col <- ifelse(i==j, "red", "green")
    drawPoly(i,j, col)
  }
}

drawPoly(1,6, "red")
drawPoly(1,7, "green")

for(i in 1:5){
  text(i+0.5, 0.5, label = paste("k = ", i, sep = ""))
}

for(i in 1:5){
  text(0.5, i+0.25, label = 6-i)
}

text(3, 7.3, label = "Training Data")
text(3, 6.3, label = "Test Data")
```

\newpage

## baye's theorem
```{r}
#head(Titanic)
```


## svm
```{r}
#svmModel <- svm()
```







